# å€ŸåŠ©Xtunerå®ç°é’ˆå¯¹æœ¬åœ°æ•°æ®é›†çš„æ¨¡å‹å¾®è°ƒä¸éƒ¨ç½²âœ¨
* å€ŸåŠ©Xtunerå®ç°é’ˆå¯¹æœ¬åœ°æ•°æ®é›†çš„æ¨¡å‹å¾®è°ƒä¸éƒ¨ç½²

  ç¼–å†™æ—¶å‚è€ƒäº†xtunerå®˜ç½‘çš„å¾®è°ƒéƒ¨ç½²æµç¨‹ï¼Œ

   Xtunerå®˜ç½‘åœ°å€ï¼š[æŸ¥çœ‹è¿™é‡Œ](https://github.com/InternLM/xtuner/blob/main/README_zh-CN.md)

é€šè¿‡é«˜æ•ˆã€åŠŸèƒ½é½å…¨çš„Xtunerå·¥å…·åº“ï¼Œå¯ä»¥å¿«é€Ÿåœ°å¯¹æœ¬åœ°æ•°æ®é›†é’ˆå¯¹æ¨¡å‹è¿›è¡Œè®­ç»ƒã€‚

æœ¬ç¬”è®°çš„ä½œç”¨ä¸ºè®°å½•æ¨¡å‹è®­ç»ƒæµç¨‹ï¼Œä¾¿äºæ—¥åå‚çœ‹ï¼Œä»¥åŠæ¨èXtunerå·¥å…·çš„ä¾¿æºä¸é«˜æ•ˆç‡ï¼Œå¦‚æœæƒ³è¦æ·±å…¥ç ”ç©¶è¿˜è¯·å‚è€ƒXtunerå®˜ç½‘çš„è®ºæ–‡ã€‚

* æˆ‘ä»¬éœ€è¦å‡†å¤‡çš„æœ‰ï¼š ç¬¦åˆè®­ç»ƒçš„æ•°æ®é›†ã€é€‚åˆå¾®è°ƒçš„æ¨¡å‹ã€xtunerå·¥å…·åº“ã€‚

å…·ä½“æµç¨‹è§ä¸‹ã€‚

## ğŸ§° å®‰è£…ä¾èµ– 
**å®‰è£…xtuner**
```python
pip install -U 'xtuner[deepspeed]

```
**æˆ–è€…ä»æºç å®‰è£…**
```python
git clone https://github.com/InternLM/xtuner.git
cd xtuner
pip install -e '.[all]'
```
## ğŸ–¥ï¸ è°ƒç”¨é…ç½®ã€å¾®è°ƒ

è¿™ä¸ªæ—¶å€™ï¼Œæˆ‘ä»¬æ‰‹å¤´ä¸Šåº”è¯¥å·²ç»æœ‰äº†æ•°æ®é›†å’ŒXtunerå·¥å…·ã€‚

**åˆ—å‡ºæ”¯æŒçš„æ¨¡å‹é…ç½®**
```python
xtuner list-cfg
```
æ ¹æ®éœ€è¦çš„é…ç½®ï¼Œå¯¼å‡ºå¹¶è¿›è¡Œç›¸åº”çš„ä¿®æ”¹ã€è®¾ç½®è·¯å¾„ã€‚

```python
xtuner copy-cfg ${CONFIG_NAME} ${SAVE_PATH}
vi ${SAVE_PATH}/${CONFIG_NAME}_copy.py
```
> æ­¤å¤„å¯¹äºé…ç½®çš„ä¿®æ”¹åº”å½“ä¸ä½ æ‰€è¦è¿›è¡Œçš„å·¥ä½œç±»åˆ«ã€æŒæœ‰çš„ç®—åŠ›èµ„æºç›¸å…³ï¼Œä¾‹å¦‚ï¼šä½¿ç”¨InternLM2-7B-Chatæ¨¡å‹ï¼Œè¿›è¡ŒQLoRAå¾®è°ƒï¼šinternlm2_chat_7b_qloraï¼Œå½“ç„¶ä¹Ÿå¯ä»¥ç›´æ¥é‡‡ç”¨é…ç½®ã€‚


**å®‰è£…æ¨¡å‹ï¼Œå¹¶æµ‹è¯•æ¨¡å‹**
```python
pip install modelscope
from modelscope import snapshot_download, AutoTokenizer, AutoModelForCausalLM
import torch

# å°†æ¨¡å‹ä¸‹è½½åˆ°æŒ‡å®šçš„æ•°æ®ç›˜è·¯å¾„ï¼Œè¿™é‡Œä»¥InternLM2-7B-Chatå’Œ /root/autodl-tmpä¸ºä¾‹
model_dir = snapshot_download("Shanghai_AI_Laboratory/internlm2-chat-7b", cache_dir="/root/autodl-tmp")

# åŠ è½½åˆ†è¯å™¨å’Œæ¨¡å‹
tokenizer = AutoTokenizer.from_pretrained(model_dir, device_map="auto", trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(model_dir, device_map="auto", trust_remote_code=True, torch_dtype=torch.float16)

# è®¾ç½®æ¨¡å‹ä¸ºæ¨ç†æ¨¡å¼
model = model.eval()

# è¿›è¡Œå¯¹è¯æµ‹è¯•
response, history = model.chat(tokenizer, "ä½ å¥½", history=[])
print(response)

response, history = model.chat(tokenizer, "è¯·æä¾›ä¸‰ä¸ªç®¡ç†æ—¶é—´çš„å»ºè®®ã€‚", history=history)
print(response)
```

**å¾®è°ƒ**
```python
xtuner train ${CONFIG_NAME_OR_PATH}
```
> å¦‚æœæœŸæœ›å¾®è°ƒçš„æ•ˆç‡å¾—åˆ°ä¼˜åŒ–ï¼Œè¯·åŠ ä¸Š`--deepspeed`ï¼Œå¹¶ä¸”å¯ä»¥é™„åŠ ä¸Š`deepspeed_zero2`è¿™æ ·çš„ä¼˜åŒ–ç­–ç•¥ã€‚
XTuner å†…ç½®äº†å¤šç§ç­–ç•¥ï¼ŒåŒ…æ‹¬ ZeRO-1ã€ZeRO-2ã€ZeRO-3ã€‚ [â€”â€”å¼•è‡ªå®˜ç½‘](https://github.com/InternLM/xtuner/blob/main/README_zh-CN.md)


å¾®è°ƒç»“æŸåï¼Œåœ¨æœ¬åœ°çš„work_dirsæ–‡ä»¶å¤¹å†…ä¼šæœ‰ä¿å­˜çš„checkpoint,é€‰æ‹©å¯¹åº”é…ç½®æ–‡ä»¶å’Œcheckpointï¼ŒæŒ‡å®šä¿å­˜è·¯å¾„ï¼Œç°åœ¨æˆ‘ä»¬å°†æ¨¡å‹è½¬æ¢ä¸ºhuggingfaceæ ¼å¼ã€‚

**æœ€åï¼Œå°†å®ƒåˆå¹¶åˆ°å¤§è¯­è¨€æ¨¡å‹**

```python
xtuner convert merge./{LLMè·¯å¾„} ./{ADAPTERè·¯å¾„} ./{ä¿å­˜è·¯å¾„} --max-shard-size 2GB
```

* è¿™ä¸ªæ—¶å€™ï¼Œæˆ‘ä»¬å¯ä»¥ï¼š
  1.ç›´æ¥ä½¿ç”¨xtunerçš„å·¥å…·ä¸æ¨¡å‹å¯¹è¯ï¼Œå¿«æ·è€Œç›´æ¥ã€‚
  2.å¯¹æ¨¡å‹è¿›è¡Œéƒ¨ç½²ï¼Œä½¿ç”¨ä»»æ„æ¡†æ¶ï¼Œä¾‹å¦‚LMDeployã€‚


## ğŸ“² éƒ¨ç½² 

**å®‰è£…LMDeploy**

```python
pip install lmdeploy
```

**æ ¼å¼è½¬æ¢**
```python
lmdeploy convert ï½›åŸæ¨¡å‹ï½ï½›å¾®è°ƒæ¨¡å‹ï½
```
* åœ¨æœ¬åœ°ç”Ÿæˆworkspaceæ–‡ä»¶å¤¹ï¼Œéšåé€šè¿‡å‘½ä»¤è¡Œå¯¹è¯ã€‚


**ç›´æ¥è°ƒç”¨æœ¬åœ°æ¨¡å‹å¯¹è¯**
```python
from IPython.display import display
import ipywidgets as widgets
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# ä½¿ç”¨æœ¬åœ°è·¯å¾„
model_name_or_path = "å†™å…¥ä½ çš„è·¯å¾„"

tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(model_name_or_path, trust_remote_code=True, torch_dtype=torch.bfloat16).cuda()
model.eval()

# åˆå§‹ç³»ç»Ÿæç¤º
system_prompt = '''åœ¨è¿™é‡Œè¾“å…¥ä½ å¯¹AIçš„ç³»ç»Ÿæç¤ºè¯ã€‚'''

# åˆå§‹åŒ–å†å²è®°å½•
history = []

# åˆ›å»ºè¾“å…¥æ¡†å’ŒæŒ‰é’®
input_box = widgets.Textarea(placeholder='ä¸AIå¯¹è¯...')
output_box = widgets.Output()
send_button = widgets.Button(description='å‘é€')

# å®šä¹‰æŒ‰é’®ç‚¹å‡»äº‹ä»¶
def on_send_button_clicked(b):
    global history
    with output_box:
        user_input = input_box.value
        input_box.value = ''
        response, history = model.chat(tokenizer, user_input, meta_instruction=system_prompt, history=history)
        print('', user_input)
        print('æœºå™¨äºº:', response)

# ç»‘å®šäº‹ä»¶
send_button.on_click(on_send_button_clicked)

# æ˜¾ç¤ºå°éƒ¨ä»¶
display(input_box, send_button, output_box)
```

è¿™æ ·ï¼Œå°±å®Œæˆäº†ä¸€æ¬¡æ¨¡å‹çš„å¾®è°ƒä¸éƒ¨ç½²ï¼Œé€šè¿‡è¿™ä¸ªæµç¨‹å¯ä»¥è·å¾—ä¸€ä»½å®Œæ•´çš„æ¨¡å‹ã€‚
